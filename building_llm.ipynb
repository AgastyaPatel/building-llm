{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dafbd669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eb86bb",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "- Built on encoder - decoder model\n",
    "- Built entirely on self attention + feed forward network layers (No Recurrence and convolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf54c45",
   "metadata": {},
   "source": [
    "<img src=\"media/Transformer%20Architecture.png\" alt=\"Transformer Architecture\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25101d31",
   "metadata": {},
   "source": [
    "### Encoder (N times)\n",
    "1. Input Embedding\n",
    "2. Positional Encoding\n",
    "3. MultiHead Attention + Add & Norm\n",
    "4. Feed Forward + Add & Norm\n",
    "\n",
    "### Decoder (N times)\n",
    "1. Output Encoding\n",
    "2. Positional Encoding\n",
    "3. *Masked* MultiHead Attention + Add & Norm\n",
    "4. Cross MultiHead Attention (Learnings from Encoder are embedded) + Add & Norm\n",
    "5. Feed Forward + Add & Norm\n",
    "\n",
    "### Output\n",
    "1. Linear\n",
    "2. Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fd895d",
   "metadata": {},
   "source": [
    "# Implementing components for Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af04bd2",
   "metadata": {},
   "source": [
    "## Building Positional Encoding Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e80a12",
   "metadata": {},
   "source": [
    "Since transformer doesn't have idea about the previous sequence of knowledge and we need a way to embed the positional information about the data. So transformer paper suggest using of the Periodic functions which can help us out in maintaining the positional information while training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d342a08b",
   "metadata": {},
   "source": [
    "Referrence: \"Attention is all you need\"\n",
    "\n",
    "$$\n",
    "\\mathrm{PE}_{(pos,\\,2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_\\text{model}}}}\\right)\n",
    "$$\n",
    "$$\n",
    "\\mathrm{PE}_{(pos,\\,2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "- pos: position of the token in the sequence (0, 1, 2, …)\n",
    "- i: dimension index (0, 1, 2, …)\n",
    "- d_{model}: the total embedding dimension (e.g., 512)\n",
    "- 2i and 2i+1 split sine and cosine across even and odd indices\n",
    "\n",
    "Basically we are using two formula based on the token's position. If the token is even or odd and then we use either cos or sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cf61f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sinusoidal_Positional_Encoding(nn.Module):\n",
    "    \"\"\"\n",
    "    d_model: Embedding dimension which is same as the input embedding dimension\n",
    "    max_seq_len: the maximum length of the input sequence\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_seq_len: int = 1000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # Positional Encoding Matrix\n",
    "        pe = torch.zeros(max_seq_len, d_model)    # creating a zero matrix of shape of the maximum sequence length and the embedding dimension \n",
    "        position = torch.arrange(0, max_seq_len, dtype=torch.float).unsqueeze(1)     # creating a position matrix of shape of the maximum sequence length\n",
    "        div_term = torch.exp(torch.arrange(0, d_model, 2).float() * -(math.log(10000)/ d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)        # for even index\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)        # for odd index\n",
    "        self.register_buffer(\"pe\", pe)  # this is a buffer which becomes a parameter of the model without gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20fd6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ablation study, we can use the learned positional encoding as well\n",
    "class Learned_Positional_Encoding(nn.Module):\n",
    "    \"\"\"\n",
    "    d_model: Embedding dimension\n",
    "    max_seq_len: the maximum length of the input sequence\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_seq_len: int = 1000):\n",
    "        super().__init__()\n",
    "        self.embeddding = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor)->torch.Tensor:\n",
    "        \"\"\"\n",
    "        x(input): (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        pos = torch.arrange(seq_len, device=x.device).unsqueeze(0)\n",
    "        pos_emb = self.embeddding(pos)\n",
    "        return x + pos_emb.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b444a3",
   "metadata": {},
   "source": [
    "## Single Head Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47246714",
   "metadata": {},
   "source": [
    "Masking is present in the multi head attention module present in decoder part of the transformer model.\n",
    "This helps the model to avoid using the next tokens in the sequence for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3366043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def casual_mask(T: int, device: torch.device)->torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns a bool mask where True means *masked*\n",
    "    \"\"\"\n",
    "\n",
    "    m = torch.triu(torch.ones(T, T, device=device), diagonal=1)\n",
    "    return m.view(1,1,T,T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600e3197",
   "metadata": {},
   "source": [
    "Referrence \"Attention is all you need\"\n",
    "\n",
    "Here the attention head created is based on the scaled dot-product attention formula presented in the transformers paper  \n",
    "**Scaled Dot-Product Attention Formula** : The scaled dot-product attention computes attention scores as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( Q \\) = queries,\n",
    "- \\( K \\) = keys,\n",
    "- \\( V \\) = values,\n",
    "- \\( d_k \\) = dimensionality of keys.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782025e0",
   "metadata": {},
   "source": [
    "<img src=\"media/attention.png\" alt=\"Scaled dot product\" width=\"400\"/>  \n",
    "\n",
    "We are implementing the Scaled Dot Product (Left Diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Single_Head_Self_Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Single-Head Attention\n",
    "    args:\n",
    "        d_model: embedding dimension\n",
    "        d_k: dimension of the key, value and \n",
    "        dropout: dropout rate\n",
    "        trace_shapes: whether to trace the shapes of the tensors for debugging\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, d_k: int, dropout: float = 0.0, causal: bool = True, trace_shapes: bool = False):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k  # Store d_k as instance variable for use in forward\n",
    "        self.q = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.k = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.v = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.causal = causal\n",
    "        self.trace_shapes = trace_shapes\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):  # (B, T, d_model)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        q = self.q(x)    # (Batch_size, Seq_len, d_k)\n",
    "        k = self.k(x)    # (Batch_size, Seq_len, d_k)\n",
    "        v = self.v(x)    # (Batch_size, Seq_len, d_k)\n",
    "        if self.trace_shapes:\n",
    "            print(f\"q: {tuple(q.shape)}\")\n",
    "            print(f\"k: {tuple(k.shape)}\")\n",
    "            print(f\"v: {tuple(v.shape)}\")\n",
    "\n",
    "        # Applying the scaled dot-product attention formula\n",
    "        scale = 1.0 / math.sqrt(self.d_k)\n",
    "        attention = torch.matmul(q, k.transpose(-2, -1)) * scale   # (Batch_size, Seq_len, Seq_len)\n",
    "        \n",
    "        # Masking the upper triangle of the attention matrix\n",
    "        # Why? To prevent the model from attending to future tokens during training\n",
    "        if self.causal:\n",
    "            mask = casual_mask(seq_len, device=x.device)\n",
    "            attention = attention.masked_fill(mask.squeeze(1), float(\"-inf\"))\n",
    "        w = F.softmax(attention, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "        out = torch.matmul(w, v)   # (Batch_size, Seq_len, d_k)\n",
    "        if self.trace_shapes:\n",
    "            print(f\"Weights: {tuple(w.shape)} Out {out.shape}\")\n",
    "        return out, w\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f1195",
   "metadata": {},
   "source": [
    "## Building multi head attention module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b11640",
   "metadata": {},
   "source": [
    "<img src=\"media/attention.png\" alt=\"Scaled dot product\" width=\"400\"/>  \n",
    "\n",
    "We are now implementing the Multiple Scaled Dot Product which are stacked (Right Diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306c8567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Head_Self_Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention\n",
    "    This represent one multi head self attention block\n",
    "\n",
    "    args:\n",
    "        d_model: embedding dimension\n",
    "        n_heads: number of attention heads\n",
    "        dropout: dropout rate\n",
    "        trace_shapes: whether to trace the shapes of the tensors for debugging\n",
    "\n",
    "    note:\n",
    "        d_k (d_head here) ie, dimension of the key, value and query is same as d_model / n_heads\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.0, causal: bool = True, trace_shapes: bool = False):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.causal = causal\n",
    "        self.trace_shapes = trace_shapes\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)      # we need to project the input to 3 times the dimension of the input so that we can split it into query, key and value\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=False)         # after the attention computation, we need to project the output back to the original dimension\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        batch_size, seq_len, C = x.shape   # c is the embedding dimension\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.view(batch_size, seq_len, 3, self.n_heads, self.d_head)\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        if self.trace_shapes:\n",
    "            print(f\"q: {tuple(q.shape)}\")\n",
    "            print(f\"k: {tuple(k.shape)}\")\n",
    "            print(f\"v: {tuple(v.shape)}\")\n",
    "\n",
    "        scale = 1.0 / math.sqrt(self.d_head)\n",
    "        attention = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "\n",
    "        # mask out the upper triangle of the attention matrix\n",
    "        if self.causal:\n",
    "            mask = casual_mask(seq_len, device=x.device)\n",
    "            attention = attention.masked_fill(mask, float(\"-inf\"))\n",
    "        \n",
    "        w = F.softmax(attention, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "        ctx = torch.matmul(w, v)\n",
    "        if self.trace_shapes:\n",
    "            print(f\"ctx: {tuple(ctx.shape)}\")\n",
    "        ctx = ctx.transpose(1, 2)\n",
    "        # we need to make sure that the context is contiguous\n",
    "        # because the view operation is not guaranteed to be contiguous\n",
    "        # if it is not contiguous, the view operation will throw an error\n",
    "        # contiguous means that the memory is in a contiguous block\n",
    "        ctx = ctx.contiguous().view(batch_size, seq_len, C)\n",
    "        out = self.proj(ctx)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building simple feed forward network with GELU activation\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed Forward Network\n",
    "    \n",
    "    Args:\n",
    "        d_model: input/output dimension\n",
    "        d_hd : hidden dimension\n",
    "        dropout: dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_hd: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_hd)\n",
    "        self.linear2 = nn.Linear(d_hd, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x : (Batch_size, Seq_len, d_model)\n",
    "        returns : (Batch_size, Seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = self.linear1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9fffc7",
   "metadata": {},
   "source": [
    "## Building Layer normalization\n",
    "mean of all features\n",
    "$$\n",
    "\\mu = \\frac{1}{D} \\sum_{i=1}^{D} x_i, \\quad\n",
    "$$\n",
    "\n",
    "variabce of all features\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{D} \\sum_{i=1}^{D} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x_i) = \\gamma \\cdot \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf4ac847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer Normalization\n",
    "\n",
    "    Args:\n",
    "        d_model : dimension to normalize over\n",
    "        eps : using small value to avoid division by zero\n",
    "        bias : boolean to include bias or not\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "        self.bias = nn.Parameter(torch.zeros(d_model)) if bias else None\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x : (Batch_size, Seq_len, d_model)\n",
    "        returns : (Batch_size, Seq_len, d_model)\n",
    "        \"\"\"\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        normalized = normalized * self.weight\n",
    "        if self.bias is not None:\n",
    "            normalized = normalized + self.bias\n",
    "        return normalized\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd23a0b2",
   "metadata": {},
   "source": [
    "we can use nn.LayerNormalization from pytorch to implement the layernormalization directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d576c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LayerNorm = nn.LayerNorm(d_model, eps=1e-5, elementwise_affine=True) \n",
    "# elementwise_affine = True means that the layer normalization will have learnable parameters ie, weights and biases\n",
    "# if we set it to False, the layer normalization will not have any learnable parameters and it will be a simple layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d08c1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Connection(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual connection with layer normalization.\n",
    "    It supports both pre normalization and post normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, bias: bool = True, pre_norm: bool = True):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model, eps=1e-5, elementwise_affine=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pre_norm = pre_norm\n",
    "        \n",
    "    def forward(self, x, sublayer) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply residual connection to any sublayer with the same size\n",
    "\n",
    "        Args:\n",
    "            x: input tensor (batch_size, seq_len, d_model)\n",
    "            sublayer: function that takes x and returns tensor of same shape\n",
    "        Returns:\n",
    "            output: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        if self.pre_norm:\n",
    "            return x + self.dropout(sublayer(self.norm(x)))\n",
    "        else:\n",
    "            return self.norm(x + self.dropout(sublayer(x)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78faa5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    This is a convenience wrapper that combines residual connection and layer norm.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer) -> torch.Tensor:\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ca1a1",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c91b14",
   "metadata": {},
   "source": [
    "```\n",
    "Input: x (batch_size, seq_len, d_model)\n",
    "    ↓\n",
    "1. Attention + Residual:\n",
    "   x = x + dropout(self_attention(layer_norm(x)))\n",
    "    ↓  \n",
    "2. Feed Forward + Residual:\n",
    "   x = x + dropout(feed_forward(layer_norm(x)))\n",
    "    ↓\n",
    "Output: x (batch_size, seq_len, d_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc2ea1f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
